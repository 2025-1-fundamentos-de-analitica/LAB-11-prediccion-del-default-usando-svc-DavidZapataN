{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7831dc3f",
   "metadata": {},
   "source": [
    "# Predicción de Default usando SVC\n",
    "\n",
    "Este notebook implementa un modelo de Support Vector Classifier (SVC) para predecir el default de tarjetas de crédito.\n",
    "\n",
    "## Contenido:\n",
    "1. Importación de librerías\n",
    "2. Funciones de limpieza de datos\n",
    "3. Definición del modelo y pipeline\n",
    "4. Optimización de hiperparámetros\n",
    "5. Funciones de evaluación\n",
    "6. Carga y procesamiento de datos\n",
    "7. Entrenamiento y evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b717838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import zipfile\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d1938",
   "metadata": {},
   "source": [
    "## 1. Función de Limpieza de Datos\n",
    "\n",
    "La función `clean_data` prepara los datos eliminando columnas innecesarias, renombrando variables y aplicando filtros específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6462ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Limpia y preprocesa el dataset de tarjetas de crédito.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset original\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset limpio\n",
    "    \"\"\"\n",
    "    # Creamos una copia\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Quitamos la columna ID\n",
    "    df = df.drop('ID', axis=1)\n",
    "    \n",
    "    # Renombramos la columna target\n",
    "    df = df.rename(columns={'default payment next month': 'default'})\n",
    "    \n",
    "    # Eliminamos registros con datos faltantes\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Eliminar filas donde el registro es NA en variables categóricas importantes\n",
    "    df = df[(df['EDUCATION'] != 0 ) & (df['MARRIAGE'] != 0)]\n",
    "    \n",
    "    # Para la columna EDUCATION, valores > 4 indican niveles superiores de educación\n",
    "    # Agrupamos estos valores en la categoría \"others\" (4)\n",
    "    df.loc[df['EDUCATION'] > 4, 'EDUCATION'] = 4\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e196b5",
   "metadata": {},
   "source": [
    "## 2. Definición del Pipeline del Modelo\n",
    "\n",
    "Se define un pipeline que incluye preprocesamiento, reducción de dimensionalidad y clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbdb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \"\"\"\n",
    "    Crea y retorna el pipeline del modelo de machine learning.\n",
    "    \n",
    "    Returns:\n",
    "        Pipeline: Pipeline con preprocesamiento y clasificador SVC\n",
    "    \"\"\"\n",
    "    # Variables categóricas para OneHotEncoding\n",
    "    categories = ['SEX', 'EDUCATION', 'MARRIAGE']  \n",
    "    \n",
    "    # Variables numéricas para estandarización\n",
    "    numerics = [\n",
    "        \"LIMIT_BAL\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\",\n",
    "        \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \n",
    "        \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\",\"PAY_AMT6\"\n",
    "    ]\n",
    "\n",
    "    # Preprocesamiento de variables\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categories),\n",
    "            ('scaler', StandardScaler(), numerics)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Selección de características más relevantes\n",
    "    selectkbest = SelectKBest(score_func=f_classif)\n",
    "\n",
    "    # Pipeline completo\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('pca', PCA()),  # Reducción de dimensionalidad\n",
    "        (\"selectkbest\", selectkbest),\n",
    "        ('classifier', SVC(kernel='rbf', random_state=42))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347fbc1",
   "metadata": {},
   "source": [
    "## 3. Optimización de Hiperparámetros\n",
    "\n",
    "Función para optimizar los hiperparámetros del modelo usando GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6f68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(model, n_splits, x_train, y_train, scoring):\n",
    "    \"\"\"\n",
    "    Optimiza los hiperparámetros del modelo usando validación cruzada.\n",
    "    \n",
    "    Args:\n",
    "        model: Pipeline del modelo\n",
    "        n_splits (int): Número de folds para validación cruzada\n",
    "        x_train: Características de entrenamiento\n",
    "        y_train: Variable objetivo de entrenamiento\n",
    "        scoring (str): Métrica de evaluación\n",
    "        \n",
    "    Returns:\n",
    "        GridSearchCV: Modelo optimizado\n",
    "    \"\"\"\n",
    "    # Búsqueda de parámetros con validación cruzada\n",
    "    estimator = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid = {\n",
    "            'pca__n_components': [20, 21],      # Componentes principales\n",
    "            'selectkbest__k': [12],             # Número de características seleccionadas\n",
    "            'classifier__kernel': ['rbf'],      # Tipo de kernel\n",
    "            'classifier__gamma': [0.099]        # Parámetro para el kernel RBF\n",
    "        },\n",
    "        cv=n_splits,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=False,\n",
    "        scoring=scoring\n",
    "    )\n",
    "    \n",
    "    # Entrenamos el modelo con los mejores parámetros\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23eae3",
   "metadata": {},
   "source": [
    "## 4. Funciones de Evaluación del Modelo\n",
    "\n",
    "Funciones para calcular métricas de rendimiento y matrices de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e11383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcula métricas de rendimiento para conjuntos de entrenamiento y prueba.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        x_train, y_train: Datos de entrenamiento\n",
    "        x_test, y_test: Datos de prueba\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Métricas de entrenamiento y prueba\n",
    "    \"\"\"\n",
    "    # Realizamos las predicciones\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Métricas para conjunto de entrenamiento\n",
    "    train_metrics = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'train',\n",
    "        'precision': precision_score(y_train, y_train_pred, average='binary'),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "        'recall': recall_score(y_train, y_train_pred, average='binary'),\n",
    "        'f1_score': f1_score(y_train, y_train_pred, average='binary')\n",
    "    }\n",
    "\n",
    "    # Métricas para conjunto de prueba\n",
    "    test_metrics = {\n",
    "        'type': 'metrics',\n",
    "        'dataset': 'test',\n",
    "        'precision': precision_score(y_test, y_test_pred, average='binary'),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred, average='binary'),\n",
    "        'f1_score': f1_score(y_test, y_test_pred, average='binary')\n",
    "    }\n",
    "\n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf5843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(model, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Calcula las matrices de confusión para conjuntos de entrenamiento y prueba.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        x_train, y_train: Datos de entrenamiento\n",
    "        x_test, y_test: Datos de prueba\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Matrices de confusión de entrenamiento y prueba\n",
    "    \"\"\"\n",
    "    # Realizamos las predicciones\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculamos los valores de la matriz de confusión\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    tn_train, fp_train, fn_train, tp_train = cm_train.ravel()\n",
    "\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n",
    "\n",
    "    # Matriz de confusión para entrenamiento\n",
    "    train_matrix = {\n",
    "        'type': 'cm_matrix',\n",
    "        'dataset': 'train', \n",
    "        'true_0': {\n",
    "            'predicted_0': int(tn_train),\n",
    "            'predicted_1': int(fp_train)\n",
    "        },\n",
    "        'true_1': {\n",
    "            'predicted_0': int(fn_train),\n",
    "            'predicted_1': int(tp_train)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Matriz de confusión para prueba\n",
    "    test_matrix = {\n",
    "        'type': 'cm_matrix',\n",
    "        'dataset': 'test', \n",
    "        'true_0': {\n",
    "            'predicted_0': int(tn_test),\n",
    "            'predicted_1': int(fp_test)\n",
    "        },\n",
    "        'true_1': {\n",
    "            'predicted_0': int(fn_test),\n",
    "            'predicted_1': int(tp_test)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab34aa4",
   "metadata": {},
   "source": [
    "## 5. Funciones de Guardado\n",
    "\n",
    "Funciones para guardar el modelo entrenado y las métricas calculadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f668e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    \"\"\"\n",
    "    Guarda el modelo entrenado en formato comprimido.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado a guardar\n",
    "    \"\"\"\n",
    "    # Crear las carpetas si no existen\n",
    "    os.makedirs('files/models', exist_ok=True)\n",
    "\n",
    "    with gzip.open('files/models/model.pkl.gz', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def save_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Guarda las métricas en formato JSON.\n",
    "    \n",
    "    Args:\n",
    "        metrics (list): Lista de diccionarios con métricas\n",
    "    \"\"\"\n",
    "    # Crear las carpetas si no existen\n",
    "    os.makedirs('files/output', exist_ok=True)\n",
    "\n",
    "    with open(\"files/output/metrics.json\", \"w\") as f:\n",
    "        for metric in metrics:\n",
    "            json_line = json.dumps(metric)\n",
    "            f.write(json_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09280c12",
   "metadata": {},
   "source": [
    "## 6. Ejecución Principal\n",
    "\n",
    "### Carga y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe70f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio actual: c:\\Users\\david\\LAB-11-prediccion-del-default-usando-svc-DavidZapataN\\homework\n",
      "Cambiado a: c:\\Users\\david\\LAB-11-prediccion-del-default-usando-svc-DavidZapataN\n",
      "Archivo test existe: True\n",
      "Archivo train existe: True\n",
      "Dataset de prueba cargado: (9000, 25)\n",
      "Dataset de entrenamiento cargado: (21000, 25)\n"
     ]
    }
   ],
   "source": [
    "# Verificar y establecer el directorio de trabajo\n",
    "import os\n",
    "print(f\"Directorio actual: {os.getcwd()}\")\n",
    "\n",
    "# Cambiar al directorio del proyecto si es necesario\n",
    "project_dir = r'c:\\Users\\david\\LAB-11-prediccion-del-default-usando-svc-DavidZapataN'\n",
    "if os.getcwd() != project_dir:\n",
    "    os.chdir(project_dir)\n",
    "    print(f\"Cambiado a: {os.getcwd()}\")\n",
    "\n",
    "# Nombres de los archivos de datos\n",
    "file_Test = 'files/input/test_data.csv.zip'\n",
    "file_Train = 'files/input/train_data.csv.zip'\n",
    "\n",
    "# Verificar que los archivos existen\n",
    "print(f\"Archivo test existe: {os.path.exists(file_Test)}\")\n",
    "print(f\"Archivo train existe: {os.path.exists(file_Train)}\")\n",
    "\n",
    "# Lectura del archivo de prueba\n",
    "with zipfile.ZipFile(file_Test, 'r') as zip:\n",
    "    with zip.open('test_default_of_credit_card_clients.csv') as f:\n",
    "        df_Test = pd.read_csv(f)\n",
    "\n",
    "# Lectura del archivo de entrenamiento\n",
    "with zipfile.ZipFile(file_Train, 'r') as zip:\n",
    "    with zip.open('train_default_of_credit_card_clients.csv') as f:\n",
    "        df_Train = pd.read_csv(f)\n",
    "\n",
    "print(f\"Dataset de prueba cargado: {df_Test.shape}\")\n",
    "print(f\"Dataset de entrenamiento cargado: {df_Train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba7acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de los datasets\n",
    "df_Test = clean_data(df_Test)\n",
    "df_Train = clean_data(df_Train)\n",
    "\n",
    "# División de los datasets en características (X) y variable objetivo (y)\n",
    "x_train, y_train = df_Train.drop('default', axis=1), df_Train['default']\n",
    "x_test, y_test = df_Test.drop('default', axis=1), df_Test['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b2a56",
   "metadata": {},
   "source": [
    "### Entrenamiento y Optimización del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f372dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del pipeline del modelo\n",
    "model_pipeline = model()\n",
    "\n",
    "# Optimización de hiperparámetros con validación cruzada\n",
    "model_pipeline = hyperparameters(model_pipeline, 10, x_train, y_train, 'balanced_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc75607",
   "metadata": {},
   "source": [
    "### Evaluación y Guardado de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232f45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado del modelo entrenado\n",
    "save_model(model_pipeline)\n",
    "\n",
    "# Cálculo de métricas de rendimiento\n",
    "train_metrics, test_metrics = metrics(model_pipeline, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Cálculo de matrices de confusión\n",
    "train_matrix, test_matrix = matrix(model_pipeline, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Guardado de todas las métricas calculadas\n",
    "save_metrics([train_metrics, test_metrics, train_matrix, test_matrix])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
